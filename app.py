import streamlit as st
import json
import os
from dotenv import load_dotenv

# ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì§€ì‹œì‚¬í•­ ë°˜ì˜)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="FAQ ì§€ëŠ¥í˜• ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

# --- 2. ë°ì´í„° ì¤€ë¹„ ë° ë²¡í„° DB (ìºì‹± ì²˜ë¦¬) ---
@st.cache_resource
def get_vector_db():
    file_path = 'data/faq_chatbot_data.json'
    
    if not os.path.exists(file_path):
        st.error(f"'{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
        
    with open(file_path, 'r', encoding='utf-8') as f:
        faq_data = json.load(f)
    
    documents = []
    for item in faq_data:
        # ì§€ì‹œì‚¬í•­ ì¤€ìˆ˜: ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê²°í•©í•˜ì—¬ ì„ë² ë”©
        combined_content = f"ì§ˆë¬¸: {item['question']}\në‹µë³€: {item['answer']}"
        doc = Document(
            page_content=combined_content, 
            metadata={"answer": item['answer'], "question": item['question']}
        )
        documents.append(doc)
    
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db = FAISS.from_documents(documents, embeddings)
    return vector_db, faq_data

# --- 3. ë©”ì¸ ë¡œì§ ---
def main():
    st.title("ğŸ¢ ê³ ê°ì§€ì› ì§€ëŠ¥í˜• FAQ ì„¼í„°")
    st.markdown("ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì„ ì„ íƒí•˜ê±°ë‚˜, ê¶ê¸ˆí•œ ì ì„ ì§ì ‘ ë¬¼ì–´ë³´ì„¸ìš”.")
    
    if not api_key:
        st.error("`.env` íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")
        return

    # DB ë° ë°ì´í„° ë¡œë“œ
    vector_db, raw_faq = get_vector_db()
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})

    # UI ë ˆì´ì•„ì›ƒ ë¶„ë¦¬
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“‹ FAQ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„ íƒ")
        faq_questions = [f["question"] for f in raw_faq]
        selected_faq = st.selectbox("ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”:", ["ì„ íƒ ì•ˆ í•¨"] + faq_questions)

    with col2:
        st.subheader("âœï¸ ì§ì ‘ ì§ˆë¬¸ ì…ë ¥")
        user_query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ë°°ì†¡ì€ ë³´í†µ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?")

    # ìµœì¢… ì§ˆë¬¸ ê²°ì •
    final_query = ""
    if user_query: # ì§ì ‘ ì…ë ¥ì´ ìˆìœ¼ë©´ ìš°ì„ ìˆœìœ„
        final_query = user_query
    elif selected_faq != "ì„ íƒ ì•ˆ í•¨":
        final_query = selected_faq

    if final_query:
        st.write(f"**ğŸ” ì§ˆë¬¸ ë‚´ìš©:** {final_query}")
        
        # --- 4. RAG ì²´ì¸ êµ¬ì„± (LCEL ìµœì‹  ë¬¸ë²•) ---
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤. ì œê³µëœ [FAQ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ë‚´ìš©ì€ ìƒë‹´ì› ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤(1588-0000)."ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.

        [FAQ ì •ë³´]
        {context}

        ì§ˆë¬¸: {input}
        ë‹µë³€:
        """)

        # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ í•©ì³ì£¼ëŠ” í•¨ìˆ˜
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # LCEL íŒŒì´í”„ë¼ì¸
        rag_chain = (
            {"context": retriever | format_docs, "input": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        # ë‹µë³€ ìƒì„±
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
            try:
                # 1. ì±—ë´‡ ë‹µë³€ ìƒì„±
                response = rag_chain.invoke(final_query)
                
                st.success("### ğŸ“¢ ìƒë‹´ì› ë‹µë³€")
                st.write(response)

                # 2. ê²€ìƒ‰ ê·¼ê±° í™•ì¸ (ì—ëŸ¬ í•´ê²°: get_relevant_documents -> invoke)
                with st.expander("ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ FAQ ë°ì´í„° (ê·¼ê±°)"):
                    # ìµœì‹  ë²„ì „ì—ì„œëŠ” invokeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    relevant_docs = retriever.invoke(final_query)
                    for i, doc in enumerate(relevant_docs):
                        st.info(f"**ê´€ë ¨ ì •ë³´ {i+1}**\n\n{doc.page_content}")
            
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    main()